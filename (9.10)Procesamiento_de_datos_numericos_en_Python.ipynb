{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento de Datos Numéricos en Python\n",
        "\n",
        "En este notebook, exploraremos varias técnicas para el procesamiento de datos numéricos, que son fundamentales en el campo de la ciencia de datos y la inteligencia artificial. Estas técnicas incluyen el escalamiento y la transformación de datos, que son esenciales para preparar los datos para los algoritmos de aprendizaje automático.\n",
        "\n",
        "El procesamiento de datos numéricos es un paso crucial en cualquier flujo de trabajo de ciencia de datos. Las técnicas de escalamiento y transformación pueden mejorar la eficacia de los algoritmos de aprendizaje automático al hacer que los datos sean más manejables y relevantes para los algoritmos. Las utilidades de preprocesamiento de Scikit Learn pueden facilitar este proceso al proporcionar funciones eficientes para realizar estas transformaciones.\n",
        "\n"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "0cca5c8d-17b2-4561-9744-1bcc7daeaa28"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries\n",
        "\n",
        "First, we need to import the necessary libraries for our data processing. We will be using `numpy` for numerical operations, `pandas` for data manipulation, `matplotlib` for data visualization, and `sklearn` for machine learning tasks."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "57a71995-5029-485d-93a2-aab0ec1ecdd6"
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, linear_model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-07-07T21:13:49.391127+00:00",
          "start_time": "2023-07-07T21:13:49.233854+00:00"
        }
      },
      "id": "22aceae1-6c1e-4f85-8cfd-ed7b64cc20e2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Preparing Data\n",
        "\n",
        "Next, we will load a dataset from `sklearn` datasets. We will use the diabetes dataset for our examples. This dataset has ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "0324ed64-1f0f-4077-b78d-7ff7b0fffabf"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = datasets.load_diabetes(return_X_y=True)\n",
        "raw = X[:, None, 2]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "ExecuteTime": {
          "end_time": "2023-07-07T21:13:53.394662+00:00",
          "start_time": "2023-07-07T21:13:53.233333+00:00"
        }
      },
      "id": "177561b4-ee7c-4b08-9551-c8eca7859d71"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scaling\n",
        "\n",
        "Data scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
        "\n",
        "### Max-Min Scaling\n",
        "\n",
        "Max-Min Scaling is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max normalization.\n",
        "\n",
        "### Z-score Normalization\n",
        "\n",
        "Z-score normalization is a strategy of normalizing data that avoids this outlier issue. Each raw value is subtracted from the mean and divided by the standard deviation, resulting in a distribution of values with a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "462f06fb-fbf1-45be-ad88-ab5d5bb0e476"
    },
    {
      "cell_type": "code",
      "source": [
        "# max-min scaling\n",
        "max_raw = max(raw)\n",
        "min_raw = min(raw)\n",
        "scaled = (2*raw - max_raw -min_raw)/(max_raw - min_raw)\n",
        "\n",
        "# z-score normalization\n",
        "avg = np.average(raw)\n",
        "std = np.std(raw)\n",
        "z_scaled = (raw - avg)/std"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "7f16a433-6a47-4bbe-8941-f898ba962e95"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Models and Comparing Training Times\n",
        "\n",
        "Now, we will train linear regression models using the raw, max-min scaled, and z-score normalized data. We will compare the training times for each to see the impact of our data scaling."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "5ab0d6c6-bfc3-42a2-afd0-4297e2929741"
    },
    {
      "cell_type": "code",
      "source": [
        "# models for training\n",
        "def train_raw():\n",
        "    linear_model.LinearRegression().fit(raw, y)\n",
        "\n",
        "def train_scaled():\n",
        "    linear_model.LinearRegression().fit(scaled, y)\n",
        "\n",
        "def train_z_scaled():\n",
        "    linear_model.LinearRegression().fit(z_scaled, y)\n",
        "\n",
        "raw_time = timeit.timeit(train_raw, number = 100)\n",
        "scaled_time = timeit.timeit(train_raw, number = 100)\n",
        "z_scaled_time = timeit.timeit(train_raw, number = 100)\n",
        "print('Training time for raw data : {} '.format(raw_time))\n",
        "print('Training time for scaled data : {}'.format(scaled_time))\n",
        "print('Training time for z_scaled data : {}'.format(z_scaled_time))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "2b976f76-1d2c-4dc9-b7f3-d1554472f38f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-linear Transformations\n",
        "\n",
        "In addition to scaling, we can also apply non-linear transformations to our data. These transformations can be useful when our data has a skewed distribution. For example, we can use the `tanh` transformation to reduce the impact of outliers."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "b867e41b-9774-41d6-90f5-0485eaf15deb"
    },
    {
      "cell_type": "code",
      "source": [
        "# non-linear transformation\n",
        "tanh_transformed = np.tanh(raw)\n",
        "\n",
        "def train_tanh_transformed():\n",
        "    linear_model.LinearRegression().fit(tanh_transformed, y)\n",
        "\n",
        "tanh_transformed_time = timeit.timeit(train_tanh_transformed, number = 100)\n",
        "print('Training time for tanh_transformed data : {}'.format(tanh_transformed_time))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        }
      },
      "id": "bb209945-3875-4748-afd7-42b0fabfbeb1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we have explored various techniques for processing numerical data, including scaling and non-linear transformations. We have seen how these techniques can impact the training time of our machine learning models. It's important to remember that the choice of preprocessing techniques depends on the specific dataset and the machine learning algorithm being used. Therefore, it's always a good idea to experiment with different preprocessing techniques to find the best approach for your specific task."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "id": "36434126-a1e3-43c9-a26e-9c9c784c526e"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "f0e13bb1-97d4-53ef-a51a-6fbc3e1ef9f1",
        "openai_ephemeral_user_id": "cee38adb-9301-598b-8ee8-61d323f36f78"
      }
    },
    "noteable": {
      "last_transaction_id": "12d0402e-67d0-4dd3-988b-de6587aef784",
      "last_delta_id": "12d0402e-67d0-4dd3-988b-de6587aef784"
    },
    "selected_hardware_size": "small",
    "nteract": {
      "version": "noteable@2.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
